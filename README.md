# ğŸ¤– AI Receptionist Chatbot â€” Johnston Group (Rasa + LM Studio + Whisper)

This project is a multimodal, locally-hosted AI receptionist designed for Johnston Group Australia. It combines natural language understanding (NLU), local large language models (LLMs), speech-to-text (Whisper), and a custom UI.

---

## ğŸ“¦ Features

- **Text + Voice Input** via Gradio UI
- **Intent Detection** using Rasa NLU
- **LLM Responses** using Mistral 7B via LM Studio (OpenAI-compatible API)
- **Audio Transcription** using Whisper
- **Custom Branding Context** for multiple companies (e.g., Liftequipt, NWTIS)
- **Dockerized** for reproducibility

---

## ğŸ§  Architecture

```
[User Text/Speech] â†’ [Gradio UI] â†’ [Rasa NLU] â†’ [Intent]
                                     â†“
                                 [action_query_llm]
                                     â†“
                            [agent.py + LM Studio API]
                                     â†“
                                 [Mistral Model]
```

---

## ğŸ—‚ï¸ Project Structure

```
AI_Reception_Agent_Project/
â”œâ”€â”€ app.py                   # Gradio UI for chat and audio
â”œâ”€â”€ agent.py                 # Brand-aware prompt manager + LLM query
â”œâ”€â”€ whisper_utils.py         # Audio â†’ text transcription via Whisper
â”œâ”€â”€ actions.py               # Rasa custom action
â”œâ”€â”€ domain.yml               # Intents, responses, actions
â”œâ”€â”€ rules.yml                # Rules for handling intents
â”œâ”€â”€ config.yml               # NLU pipeline + policies
â”œâ”€â”€ requirements.txt         # Whisper + UI deps
â”œâ”€â”€ rasa_requirements.txt    # Rasa SDK + server
â”œâ”€â”€ Dockerfile               # Unified Docker container build
â”œâ”€â”€ docker-compose.yml       # Run rasa, actions, and UI in one command
â””â”€â”€ data/
    â”œâ”€â”€ faq_data.json        # Structured FAQs for multiple brands
    â””â”€â”€ nlu.yml              # Rasa training examples
```

---

## ğŸš€ Running the Project (Locally with Docker)

### 1. âœ… Prerequisites
- [Docker Desktop](https://www.docker.com/products/docker-desktop)
- [LM Studio](https://lmstudio.ai) running with Mistral and REST API enabled at `http://localhost:1234`

### 2. ğŸ”§ Build and Start
```bash
docker compose build --no-cache
docker compose up
```

### 3. ğŸ§ª Test the Chatbot
- Open browser: [http://localhost:7860](http://localhost:7860)
- Speak or type a question
- The bot answers using Rasa + LLM

---

## ğŸ§© FAQs

- **How does LLM integration work?**
  The `agent.py` module formats the user question and sends it to LM Studioâ€™s `/v1/chat/completions` endpoint using the OpenAI-compatible API.

- **How is audio handled?**
  Gradio records audio and sends it as a NumPy array to Whisper for transcription.

- **What model does it use?**
  Mistral 7B GGUF format running locally through LM Studio (compatible with OpenAI API).

---

## ğŸ“¬ Need Help?

Feel free to reach out for deployment support, cloud hosting, or adding new company data to the bot.