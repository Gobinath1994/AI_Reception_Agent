# ğŸ¤– AI Receptionist Chatbot

An intelligent, voice- and text-enabled chatbot assistant designed for organizations with multiple business units. This assistant uses local large language models (like Mistral via LM Studio), offline speech-to-text using Whisper, and a sleek Gradio UI for interaction â€” all running 100% locally for privacy and control.

---

## ğŸ—ï¸ Features

- âœ… Voice and text input via Gradio
- âœ… Offline Whisper-based audio transcription
- âœ… Integration with local Mistral model using LM Studio (OpenAI-compatible API)
- âœ… JSON-based FAQ knowledge base per brand or department
- âœ… Custom brand detection and contextual prompt generation
- âœ… Rasa backend for intent detection, routing, and fallback handling
- âœ… Fully Dockerized deployment with separate services for Rasa, Actions, and UI

---

## ğŸ“ Project Structure

```
AI_Reception_Agent/
â”œâ”€â”€ app.py                 # Gradio UI
â”œâ”€â”€ agent.py               # Prompt router and LLM interface
â”œâ”€â”€ actions.py             # Rasa custom action to call LLM
â”œâ”€â”€ whisper_utils.py       # Whisper STT utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ faq_data.json      # Structured FAQ data per company
â”œâ”€â”€ models/                # Rasa models (ignored in Git)
â”œâ”€â”€ config.yml             # Rasa NLU pipeline configuration
â”œâ”€â”€ domain.yml             # Intents, responses, actions
â”œâ”€â”€ rules.yml              # Rule-based policy handling
â”œâ”€â”€ rasa_requirements.txt  # Rasa-specific dependencies
â”œâ”€â”€ requirements.txt       # General project requirements
â”œâ”€â”€ Dockerfile             # Single Docker image definition
â”œâ”€â”€ docker-compose.yml     # Multi-container setup
â””â”€â”€ README.md              # This file
```

---

## ğŸ§  Getting Started

### 1. Clone and Set Up

```bash
git clone https://github.com/YOUR_ORG/AI_Reception_Agent.git
cd AI_Reception_Agent

conda create -n ai_agent python=3.8 -y
conda activate ai_agent

pip install -r requirements.txt
pip install -r rasa_requirements.txt
```

### 2. Run Locally with Docker

```bash
docker compose up --build
```

Services:
- `http://localhost:7860` â†’ Gradio UI
- `http://localhost:5005` â†’ Rasa
- `http://localhost:5055` â†’ Rasa actions

---

## ğŸ—‚ï¸ FAQ Management

Update `data/faq_data.json` to define your business units:

```json
{
  "your_org": {
    "overview": "We are a multi-brand service provider.",
    "contact": {"email": "info@example.com"},
    "companies": {
      "brand_a": {
        "name": "Brand A",
        "services": ["Training", "Inspections"],
        "locations": ["Melbourne", "Sydney"],
        "phone": "1234 5678"
      }
    }
  }
}
```

---

## ğŸ¯ Custom Rasa Intents and Routing

Edit `domain.yml`, `rules.yml`, and `config.yml` to define how intents like `faq_services` or `faq_company_overview` are routed to the `action_query_llm`.

---

## ğŸ§  Local LLM Setup (LM Studio)

1. Install [LM Studio](https://lmstudio.ai)
2. Load a compatible model (e.g., Mistral-7B)
3. Enable the OpenAI-compatible server at `http://localhost:1234`
4. Ensure `agent.py` and `actions.py` are pointing to this URL

---

## ğŸ™ï¸ Voice Input

Whisper is used for offline speech-to-text.

Dependencies:
- `ffmpeg`
- `torch`
- `whisper`

Microphone input is handled directly via Gradio's `Audio` component.

---

## ğŸ§¾ License

MIT License â€” Youâ€™re free to use, modify, and redistribute this tool.

---

## ğŸ› ï¸ Contributing

Feel free to open issues or submit PRs for improvements. Add integrations (e.g., CRM, appointment booking), or extend with multilingual support.

---

Built with â¤ï¸ for teams who want AI automation without sending data to the cloud.